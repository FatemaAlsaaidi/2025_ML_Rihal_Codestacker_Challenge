{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "38d15dd1-0ceb-4159-aa71-cdcf0009e85b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "level4_severity.py level4_extraction.py streamlit_app_level4.py\n"
     ]
    }
   ],
   "source": [
    "# Create Level 4 implementation files: extractor, severity, and Streamlit app (STATIC MODEL).\n",
    "from pathlib import Path\n",
    "from textwrap import dedent\n",
    "import tempfile\n",
    "\n",
    "# 1) level4_severity.py\n",
    "severity_py = dedent('''\n",
    "# level4_severity.py\n",
    "# -------------------\n",
    "# Simple severity mapping based on crime Category.\n",
    "\n",
    "SEVERITY_MAP = {\n",
    "    # Severity 1\n",
    "    \"NON-CRIMINAL\": 1, \"SUSPICIOUS OCCURRENCE\": 1, \"MISSING PERSON\": 1, \"RUNAWAY\": 1, \"RECOVERED VEHICLE\": 1,\n",
    "    # Severity 2\n",
    "    \"WARRANTS\": 2, \"OTHER OFFENSES\": 2, \"VANDALISM\": 2, \"TRESPASS\": 2, \"DISORDERLY CONDUCT\": 2, \"BAD CHECKS\": 2,\n",
    "    # Severity 3\n",
    "    \"LARCENY/THEFT\": 3, \"VEHICLE THEFT\": 3, \"FORGERY/COUNTERFEITING\": 3, \"DRUG/NARCOTIC\": 3,\n",
    "    \"STOLEN PROPERTY\": 3, \"FRAUD\": 3, \"BRIBERY\": 3, \"EMBEZZLEMENT\": 3,\n",
    "    # Severity 4\n",
    "    \"ROBBERY\": 4, \"WEAPON LAWS\": 4, \"BURGLARY\": 4, \"EXTORTION\": 4,\n",
    "    # Severity 5\n",
    "    \"KIDNAPPING\": 5, \"ARSON\": 5\n",
    "}\n",
    "\n",
    "def assign_severity(category: str) -> int:\n",
    "    if not category:\n",
    "        return 0\n",
    "    cat = str(category).strip().upper()\n",
    "    return SEVERITY_MAP.get(cat, 0)\n",
    "''').strip()\n",
    "\n",
    "# 2) level4_extraction.py\n",
    "extraction_py = dedent('''\n",
    "# level4_extraction.py\n",
    "# ---------------------\n",
    "from __future__ import annotations\n",
    "import re\n",
    "from typing import List, Dict, Any\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "\n",
    "# Optional dependencies handled gracefully\n",
    "def _extract_text_pdfplumber(path: str) -> str:\n",
    "    try:\n",
    "        import pdfplumber\n",
    "    except Exception:\n",
    "        return \"\"\n",
    "    text_parts = []\n",
    "    with pdfplumber.open(path) as pdf:\n",
    "        for page in pdf.pages:\n",
    "            try:\n",
    "                text_parts.append(page.extract_text() or \"\")\n",
    "            except Exception:\n",
    "                continue\n",
    "    return \"\\\\n\".join(text_parts).strip()\n",
    "\n",
    "def _extract_text_pypdf(path: str) -> str:\n",
    "    try:\n",
    "        from PyPDF2 import PdfReader\n",
    "    except Exception:\n",
    "        return \"\"\n",
    "    text_parts = []\n",
    "    try:\n",
    "        reader = PdfReader(path)\n",
    "        for page in reader.pages:\n",
    "            try:\n",
    "                text_parts.append(page.extract_text() or \"\")\n",
    "            except Exception:\n",
    "                continue\n",
    "    except Exception:\n",
    "        return \"\"\n",
    "    return \"\\\\n\".join(text_parts).strip()\n",
    "\n",
    "def extract_text_from_pdf(path: str) -> str:\n",
    "    # Try multiple extractors; return the first with sufficient words\n",
    "    for fn in (_extract_text_pdfplumber, _extract_text_pypdf):\n",
    "        txt = fn(path)\n",
    "        if txt and len(txt.split()) > 3:\n",
    "            return txt\n",
    "    return \"\"\n",
    "\n",
    "# (Optional) OCR for scanned PDFs if pytesseract & pillow are installed\n",
    "def ocr_pdf_first_n_pages(path: str, n_pages: int = 3, dpi: int = 300) -> str:\n",
    "    try:\n",
    "        import pytesseract\n",
    "        from pdf2image import convert_from_path\n",
    "    except Exception:\n",
    "        return \"\"\n",
    "    text_parts = []\n",
    "    try:\n",
    "        images = convert_from_path(path, dpi=dpi, first_page=1, last_page=n_pages)\n",
    "        for img in images:\n",
    "            try:\n",
    "                text_parts.append(pytesseract.image_to_string(img) or \"\")\n",
    "            except Exception:\n",
    "                continue\n",
    "    except Exception:\n",
    "        return \"\"\n",
    "    return \"\\\\n\".join(text_parts).strip()\n",
    "\n",
    "# ---------------- Parsing ----------------\n",
    "DATE_PATTERNS = [\n",
    "    r\"(\\\\b\\\\d{4}-\\\\d{2}-\\\\d{2}\\\\b)\",\n",
    "    r\"(\\\\b\\\\d{2}/\\\\d{2}/\\\\d{4}\\\\b)\",\n",
    "    r\"(\\\\b\\\\d{1,2}-\\\\d{1,2}-\\\\d{2,4}\\\\b)\",\n",
    "]\n",
    "TIME_PATTERNS = [r\"(\\\\b\\\\d{1,2}:\\\\d{2}(?::\\\\d{2})?\\\\s?(?:AM|PM|am|pm)?\\\\b)\"]\n",
    "PD_DISTRICT_PATTERNS = [r\"\\\\b(PD\\\\s*District|Police\\\\s*District|District)[:\\\\-\\\\s]+([A-Za-z\\\\s]+)\\\\b\"]\n",
    "ADDRESS_PATTERNS = [r\"\\\\bAddress[:\\\\-\\\\s]+(.+)$\", r\"\\\\bLocation[:\\\\-\\\\s]+(.+)$\"]\n",
    "COORD_PATTERNS = [\n",
    "    r\"Latitude\\\\s*\\\\(?Y\\\\)?[:\\\\-\\\\s]*(-?\\\\d+\\\\.\\\\d+)\\\\b.*Longitude\\\\s*\\\\(?X\\\\)?[:\\\\-\\\\s]*(-?\\\\d+\\\\.\\\\d+)\\\\b\",\n",
    "    r\"\\\\bLat[:\\\\-\\\\s]*(-?\\\\d+\\\\.\\\\d+)[,\\\\s]+Lon[g]?[:\\\\-\\\\s]*(-?\\\\d+\\\\.\\\\d+)\\\\b\",\n",
    "]\n",
    "\n",
    "def _first_group(patterns: List[str], text: str, flags=re.MULTILINE) -> str | None:\n",
    "    for pat in patterns:\n",
    "        m = re.search(pat, text, flags)\n",
    "        if m:\n",
    "            return m.group(m.lastindex or 0)\n",
    "    return None\n",
    "\n",
    "def parse_fields_from_text(text: str) -> Dict[str, Any]:\n",
    "    out = {\n",
    "        \"Descript\": text.strip(),\n",
    "        \"Dates\": None,\n",
    "        \"Address\": None,\n",
    "        \"PdDistrict\": None,\n",
    "        \"Longitude (X)\": None,\n",
    "        \"Latitude (Y)\": None,\n",
    "    }\n",
    "    # Parse date and (optionally) time\n",
    "    date_raw = _first_group(DATE_PATTERNS, text) or \"\"\n",
    "    time_raw = _first_group(TIME_PATTERNS, text) or \"\"\n",
    "    date_val = None\n",
    "    for fmt in (\"%Y-%m-%d\", \"%m/%d/%Y\", \"%m-%d-%Y\", \"%m-%d-%y\"):\n",
    "        try:\n",
    "            if date_raw:\n",
    "                date_val = datetime.strptime(date_raw, fmt)\n",
    "                break\n",
    "        except ValueError:\n",
    "            continue\n",
    "    if date_val and time_raw:\n",
    "        time_clean = time_raw.upper().replace(\" \", \"\")\n",
    "        for tfmt in (\"%H:%M\", \"%H:%M:%S\", \"%I:%M%p\", \"%I:%M:%S%p\"):\n",
    "            try:\n",
    "                t = datetime.strptime(time_clean, tfmt)\n",
    "                date_val = date_val.replace(hour=t.hour, minute=t.minute, second=getattr(t, \"second\", 0))\n",
    "                break\n",
    "            except ValueError:\n",
    "                continue\n",
    "    out[\"Dates\"] = date_val.isoformat(sep=\" \") if date_val else None\n",
    "\n",
    "    # Parse PD district\n",
    "    pd_m = None\n",
    "    for pat in PD_DISTRICT_PATTERNS:\n",
    "        m = re.search(pat, text, flags=re.IGNORECASE)\n",
    "        if m:\n",
    "            pd_m = m.group(m.lastindex or 0)\n",
    "            break\n",
    "    out[\"PdDistrict\"] = pd_m.strip() if pd_m else None\n",
    "\n",
    "    # Parse address (take first line only if multi-line)\n",
    "    addr = None\n",
    "    for pat in ADDRESS_PATTERNS:\n",
    "        m = re.search(pat, text, flags=re.IGNORECASE | re.MULTILINE)\n",
    "        if m:\n",
    "            addr = m.group(m.lastindex or 0)\n",
    "            if \"\\\\n\" in addr:\n",
    "                addr = addr.split(\"\\\\n\", 1)[0]\n",
    "            break\n",
    "    out[\"Address\"] = addr.strip() if addr else None\n",
    "\n",
    "    # Parse coordinates if present\n",
    "    for pat in COORD_PATTERNS:\n",
    "        m = re.search(pat, text, flags=re.IGNORECASE | re.DOTALL)\n",
    "        if m:\n",
    "            try:\n",
    "                lat = float(m.group(1)); lon = float(m.group(2))\n",
    "                out[\"Latitude (Y)\"] = float(lat)\n",
    "                out[\"Longitude (X)\"] = float(lon)\n",
    "            except Exception:\n",
    "                pass\n",
    "            break\n",
    "    return out\n",
    "\n",
    "def features_from_rows(rows: List[Dict[str, Any]]) -> pd.DataFrame:\n",
    "    # Build a clean DataFrame and derive basic time features\n",
    "    df = pd.DataFrame(rows)\n",
    "    for col in [\"Descript\", \"Address\", \"PdDistrict\"]:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].fillna(\"\").astype(str).str.strip()\n",
    "\n",
    "    if \"Dates\" in df.columns:\n",
    "        df[\"Dates\"] = pd.to_datetime(df[\"Dates\"], errors=\"coerce\")\n",
    "        df[\"Year\"] = df[\"Dates\"].dt.year\n",
    "        df[\"Month\"] = df[\"Dates\"].dt.month\n",
    "        df[\"DayOfMonth\"] = df[\"Dates\"].dt.day\n",
    "        df[\"Hour\"] = df[\"Dates\"].dt.hour\n",
    "        df[\"DayOfWeek\"] = df[\"Dates\"].dt.day_name()\n",
    "    else:\n",
    "        df[\"Dates\"] = pd.NaT\n",
    "        df[\"Year\"] = None; df[\"Month\"] = None; df[\"DayOfMonth\"] = None; df[\"Hour\"] = None; df[\"DayOfWeek\"] = None\n",
    "\n",
    "    for c in [\"Latitude (Y)\", \"Longitude (X)\"]:\n",
    "        if c in df.columns:\n",
    "            df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "\n",
    "    # Reorder columns for consistency\n",
    "    preferred_cols = [\n",
    "        \"Dates\",\"Descript\",\"Address\",\"PdDistrict\",\n",
    "        \"Latitude (Y)\",\"Longitude (X)\",\n",
    "        \"Year\",\"Month\",\"DayOfMonth\",\"Hour\",\"DayOfWeek\"\n",
    "    ]\n",
    "    preferred_in_df = [c for c in preferred_cols if c in df.columns]\n",
    "    rest = [c for c in df.columns if c not in preferred_in_df]\n",
    "    df = df[preferred_in_df + rest]\n",
    "    return df\n",
    "''').strip()\n",
    "\n",
    "# 3) streamlit_app_level4.py  (Static model loading + safer inference)\n",
    "app_py = dedent('''\n",
    "# streamlit_app_level4.py\n",
    "# -----------------------\n",
    "import io\n",
    "import numpy as np\n",
    "import joblib\n",
    "import pandas as pd\n",
    "import streamlit as st\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any\n",
    "import tempfile\n",
    "\n",
    "from level4_extraction import extract_text_from_pdf, ocr_pdf_first_n_pages, parse_fields_from_text, features_from_rows\n",
    "from level4_severity import assign_severity\n",
    "\n",
    "st.set_page_config(page_title=\"CityX - Level 4: PDF Extraction & Inference\", page_icon=\"üìÑ\", layout=\"wide\")\n",
    "st.title(\"Police Report Extraction ‚Üí Classification ‚Üí Severity\")\n",
    "\n",
    "# --------- Static model path ---------\n",
    "DEFAULT_MODEL_PATH = Path(\"../models/crime_category_text_model.pkl\")\n",
    "\n",
    "# Flexible checker: accepts a Pipeline or a dict containing a pipeline or (vectorizer + classifier)\n",
    "def _normalize_loaded_object(obj):\n",
    "    # If it's a pipeline-like with predict\n",
    "    if hasattr(obj, \"predict\"):\n",
    "        return obj\n",
    "    # If it's a dictionary\n",
    "    if isinstance(obj, dict):\n",
    "        for key in (\"pipeline\", \"model\", \"estimator\", \"clf\"):\n",
    "            if key in obj and hasattr(obj[key], \"predict\"):\n",
    "                return obj[key]\n",
    "        if \"vectorizer\" in obj and \"classifier\" in obj and hasattr(obj[\"classifier\"], \"predict\"):\n",
    "            class _VecClfWrapper:\n",
    "                def __init__(self, vectorizer, classifier):\n",
    "                    self.vectorizer = vectorizer\n",
    "                    self.classifier = classifier\n",
    "                def predict(self, texts):\n",
    "                    X = self.vectorizer.transform(texts)\n",
    "                    return self.classifier.predict(X)\n",
    "                def predict_proba(self, texts):\n",
    "                    if hasattr(self.classifier, \"predict_proba\"):\n",
    "                        X = self.vectorizer.transform(texts)\n",
    "                        return self.classifier.predict_proba(X)\n",
    "                    raise AttributeError(\"Classifier has no predict_proba\")\n",
    "                def decision_function(self, texts):\n",
    "                    if hasattr(self.classifier, \"decision_function\"):\n",
    "                        X = self.vectorizer.transform(texts)\n",
    "                        return self.classifier.decision_function(X)\n",
    "                    raise AttributeError(\"Classifier has no decision_function\")\n",
    "            return _VecClfWrapper(obj[\"vectorizer\"], obj[\"classifier\"])\n",
    "    raise ValueError(\"Unsupported model format. Expect a Pipeline or dict with 'pipeline' or ('vectorizer' + 'classifier').\")\n",
    "\n",
    "@st.cache_resource\n",
    "def load_static_model(path: Path):\n",
    "    if not path.exists():\n",
    "        return None, f\"Model not found at: {path}\"\n",
    "    try:\n",
    "        obj = joblib.load(path)\n",
    "        model = _normalize_loaded_object(obj)\n",
    "        return model, None\n",
    "    except Exception as e:\n",
    "        return None, f\"Failed to load model: {e}\"\n",
    "\n",
    "with st.sidebar:\n",
    "    st.header(\"‚öôÔ∏è Settings\")\n",
    "    # Allow user to see/override the static model path (optional)\n",
    "    model_path_str = st.text_input(\"Static model path\", value=str(DEFAULT_MODEL_PATH))\n",
    "    #use_ocr = st.toggle(\"Try OCR fallback for scanned PDFs (slower)\", value=False)\n",
    "    st.divider()\n",
    "    st.markdown(\"**Batch PDF upload**\")\n",
    "    pdf_files = st.file_uploader(\"Police report PDFs\", type=[\"pdf\"], accept_multiple_files=True, key=\"pdf_upl\")\n",
    "\n",
    "# --- Load static model once ---\n",
    "model_status = st.empty()\n",
    "clf, load_err = load_static_model(Path(model_path_str))\n",
    "#if clf is not None:\n",
    "    #model_status.success(f\" Static model loaded from: {model_path_str}\")\n",
    "#else:\n",
    "    #model_status.error(load_err or \"Could not load the static model.\")\n",
    "    #st.stop()  # No model ‚Üí stop the app early\n",
    "\n",
    "# --- Extraction ---\n",
    "st.subheader(\"1) Extract key fields from uploaded PDFs\")\n",
    "rows: List[Dict[str, Any]] = []\n",
    "\n",
    "if pdf_files:\n",
    "    progress = st.progress(0)\n",
    "    tmp_dir = Path(tempfile.gettempdir()) / \"cityx_uploads\"\n",
    "    tmp_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    for i, upl in enumerate(pdf_files):\n",
    "        progress.progress((i+1)/len(pdf_files))\n",
    "\n",
    "        tmp_path = tmp_dir / upl.name\n",
    "        # Streamlit versions differ: sometimes .getbuffer(), other times .read()\n",
    "        try:\n",
    "            data = upl.getbuffer()\n",
    "        except Exception:\n",
    "            data = upl.read()\n",
    "        with open(tmp_path, \"wb\") as f:\n",
    "            f.write(data)\n",
    "\n",
    "        text = extract_text_from_pdf(str(tmp_path))\n",
    "        if not text and use_ocr:\n",
    "            text = ocr_pdf_first_n_pages(str(tmp_path), n_pages=2, dpi=200)\n",
    "        if not text:\n",
    "            st.warning(f\"Could not extract text from {upl.name}. It might be a scanned PDF and OCR is disabled or unavailable.\")\n",
    "            continue\n",
    "\n",
    "        parsed = parse_fields_from_text(text)\n",
    "        parsed[\"__source_pdf__\"] = upl.name\n",
    "        rows.append(parsed)\n",
    "\n",
    "    progress.progress(1.0)\n",
    "\n",
    "if not rows:\n",
    "    st.info(\"Upload one or more PDF files to extract. Parsed results will appear here.\")\n",
    "else:\n",
    "    df_extracted = features_from_rows(rows)\n",
    "    st.write(\"**Extracted (editable) table** ‚Äì adjust any fields if needed before inference:\")\n",
    "    edited = st.data_editor(df_extracted, num_rows=\"dynamic\", use_container_width=True, key=\"editor_level4\")\n",
    "\n",
    "    st.subheader(\"2) Run Level-2 Classifier on Descriptions ‚Üí Predict Category & Severity\")\n",
    "    can_infer = \"Descript\" in edited.columns\n",
    "    infer_btn = st.button(\"Predict\", type=\"primary\", disabled=not can_infer)\n",
    "\n",
    "    if infer_btn and can_infer:\n",
    "        X_text = edited[\"Descript\"].fillna(\"\").astype(str).tolist()\n",
    "        y_pred = None\n",
    "        conf = None\n",
    "        try:\n",
    "            y_pred = clf.predict(X_text)\n",
    "            if hasattr(clf, \"decision_function\"):\n",
    "                arr = clf.decision_function(X_text)\n",
    "                conf = np.abs(arr) if getattr(arr, \"ndim\", 1) == 1 else arr.max(axis=1)\n",
    "            elif hasattr(clf, \"predict_proba\"):\n",
    "                arr = clf.predict_proba(X_text)\n",
    "                conf = arr.max(axis=1)\n",
    "        except Exception as e:\n",
    "            st.error(f\"Prediction failed: {e}\")\n",
    "            y_pred = None\n",
    "\n",
    "        if y_pred is None or len(y_pred) != len(edited):\n",
    "            st.error(f\"Prediction returned {0 if y_pred is None else len(y_pred)} results for {len(edited)} rows. Check model format or input.\")\n",
    "        else:\n",
    "            result = edited.copy()\n",
    "            result[\"PredictedCategory\"] = pd.Series(y_pred, index=result.index).astype(str).str.upper()\n",
    "            result[\"AssignedSeverity\"] = result[\"PredictedCategory\"].apply(assign_severity)\n",
    "            if conf is not None:\n",
    "                result[\"Confidence\"] = pd.Series(conf, index=result.index).round(3)\n",
    "\n",
    "            st.success(\"Inference complete.\")\n",
    "            st.dataframe(result, use_container_width=True)\n",
    "\n",
    "            c1, c2, c3 = st.columns(3)\n",
    "            with c1: st.metric(\"Reports\", len(result))\n",
    "            with c2: st.metric(\"Unique predicted types\", result[\"PredictedCategory\"].nunique())\n",
    "            with c3: st.metric(\"Avg confidence\", float(result.get(\"Confidence\", pd.Series([0])).mean() or 0))\n",
    "\n",
    "            st.download_button(\"Download Predictions (CSV)\", data=result.to_csv(index=False), file_name=\"predictions_level4.csv\", mime=\"text/csv\")\n",
    "            st.download_button(\"Download Predictions (JSON)\", data=result.to_json(orient=\"records\"), file_name=\"predictions_level4.json\", mime=\"application/json\")\n",
    "\n",
    "            # Quick map preview (if coordinates available)\n",
    "            try:\n",
    "                import folium\n",
    "                from streamlit_folium import folium_static\n",
    "                df_map = result.dropna(subset=[\"Latitude (Y)\", \"Longitude (X)\"])\n",
    "                if not df_map.empty:\n",
    "                    m = folium.Map(\n",
    "                        location=[df_map[\"Latitude (Y)\"].median(), df_map[\"Longitude (X)\"].median()],\n",
    "                        zoom_start=12,\n",
    "                        tiles=\"CartoDB positron\"\n",
    "                    )\n",
    "\n",
    "                    for _, r in df_map.iterrows():\n",
    "                        folium.CircleMarker(\n",
    "                            location=[r[\"Latitude (Y)\"], r[\"Longitude (X)\"]],\n",
    "                            radius=5,\n",
    "                            popup=f\"{r.get('PredictedCategory','?')} | Sev {r.get('AssignedSeverity','?')}\",\n",
    "                        ).add_to(m)\n",
    "                    st.write(\"**Map preview (if coordinates present):**\")\n",
    "                    folium_static(m, width=1000, height=450)\n",
    "                else:\n",
    "                    st.info(\"No valid coordinates to map. You can add/edit Lat/Lon in the table above.\")\n",
    "            except Exception as e:\n",
    "                st.info(f\"Map preview unavailable: {e}\")\n",
    "''').strip()\n",
    "\n",
    "# Write files to disk (same creation mechanism as your original)\n",
    "out1 = Path(\"level4_severity.py\"); out1.parent.mkdir(parents=True, exist_ok=True); out1.write_text(severity_py, encoding=\"utf-8\")\n",
    "out2 = Path(\"level4_extraction.py\"); out2.parent.mkdir(parents=True, exist_ok=True); out2.write_text(extraction_py, encoding=\"utf-8\")\n",
    "out3 = Path(\"streamlit_app_level4.py\"); out3.parent.mkdir(parents=True, exist_ok=True); out3.write_text(app_py, encoding=\"utf-8\")\n",
    "print(out1, out2, out3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d38b03d4-2ec0-4485-b127-791b48e87f8c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
