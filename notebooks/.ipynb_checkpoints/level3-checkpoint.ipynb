{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "82338dda-e894-451f-a484-394d66b39948",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "39d020fd-d462-4d0c-b77f-a175e6bc25e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = Path(\"../data/processed/cityx_clean.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "066080d5-457b-47d3-81d3-b18e1a5a869c",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA = pd.read_parquet(DATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "17d6d3d7-9c2c-43c4-bf69-a60290271025",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating time analysis figure...\n",
      "Time analysis saved ‚Üí time_analysis.html\n",
      "Creating interactive crime map...\n",
      "Crime map saved ‚Üí crime_map.html\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.express as px\n",
    "\n",
    "# =========================\n",
    "# Global constants / settings\n",
    "# =========================\n",
    "LAT_COL = \"Latitude (Y)\"\n",
    "LON_COL = \"Longitude (X)\"\n",
    "CATEGORY_COL = \"Category\"\n",
    "DESCRIPT_COL = \"Descript\"\n",
    "PDDIST_COL = \"PdDistrict\"\n",
    "DATE_COLS = [\"DateTime\", \"Dates\"]  # Will take the first available column\n",
    "DAYOFWEEK_COL = \"DayOfWeek\"\n",
    "HOUR_COL = \"Hour\"\n",
    "MONTH_COL = \"Month\"\n",
    "TIMEOFDAY_COL = \"TimeOfDay\"\n",
    "\n",
    "# Weekday order\n",
    "DAILY_ORDER = [\"Monday\",\"Tuesday\",\"Wednesday\",\"Thursday\",\"Friday\",\"Saturday\",\"Sunday\"]\n",
    "\n",
    "# =========================\n",
    "# Helper functions\n",
    "# =========================\n",
    "def ensure_time_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Ensure Hour / Month / DayOfWeek columns exist; derive them from DateTime or Dates if missing.\"\"\"\n",
    "    out = df.copy()\n",
    "    # If Hour/Month/DayOfWeek are missing, derive them from DateTime/Dates\n",
    "    if HOUR_COL not in out.columns or MONTH_COL not in out.columns or DAYOFWEEK_COL not in out.columns:\n",
    "        dt = None\n",
    "        for c in DATE_COLS:\n",
    "            if c in out.columns:\n",
    "                dt = pd.to_datetime(out[c], errors=\"coerce\")\n",
    "                break\n",
    "        if dt is None:\n",
    "            # No date column to derive from\n",
    "            missing = [HOUR_COL, MONTH_COL, DAYOFWEEK_COL]\n",
    "            still_missing = [m for m in missing if m not in out.columns]\n",
    "            if still_missing:\n",
    "                raise ValueError(f\"No time columns to derive from. Missing: {still_missing} and no {DATE_COLS} found.\")\n",
    "        else:\n",
    "            if HOUR_COL not in out.columns:\n",
    "                out[HOUR_COL] = dt.dt.hour\n",
    "            if MONTH_COL not in out.columns:\n",
    "                out[MONTH_COL] = dt.dt.to_period(\"M\").astype(str)\n",
    "            if DAYOFWEEK_COL not in out.columns:\n",
    "                out[DAYOFWEEK_COL] = dt.dt.day_name()\n",
    "    return out\n",
    "\n",
    "\n",
    "def fix_and_filter_geo(df: pd.DataFrame, max_points: int = 20000) -> pd.DataFrame:\n",
    "    \"\"\"Clean and fix Latitude/Longitude columns and sample large datasets.\"\"\"\n",
    "    g = df.copy()\n",
    "    if LAT_COL not in g.columns or LON_COL not in g.columns:\n",
    "        raise ValueError(f\"Missing geo columns: need '{LAT_COL}' and '{LON_COL}'.\")\n",
    "\n",
    "    g[LAT_COL] = pd.to_numeric(g[LAT_COL], errors=\"coerce\")\n",
    "    g[LON_COL] = pd.to_numeric(g[LON_COL], errors=\"coerce\")\n",
    "\n",
    "    # Quick diagnostic: if absolute median of coordinate > 90, columns might be swapped\n",
    "    lat_med = g[LAT_COL].abs().median(skipna=True)\n",
    "    lon_med = g[LON_COL].abs().median(skipna=True)\n",
    "\n",
    "    # Auto-fix if columns seem swapped (latitude must always be ‚â§ 90)\n",
    "    if (lat_med is not np.nan) and (lon_med is not np.nan):\n",
    "        if (lat_med > 90) and (lon_med <= 90):\n",
    "            # Swap columns\n",
    "            g[[LAT_COL, LON_COL]] = g[[LON_COL, LAT_COL]]\n",
    "\n",
    "    # Light cleaning for valid coordinate ranges\n",
    "    g = g.dropna(subset=[LAT_COL, LON_COL])\n",
    "    g = g[g[LAT_COL].between(-90, 90) & g[LON_COL].between(-180, 180)]\n",
    "\n",
    "    # Random sample if dataset is too large\n",
    "    if len(g) > max_points:\n",
    "        g = g.sample(max_points, random_state=42)\n",
    "\n",
    "    if g.empty:\n",
    "        raise ValueError(\"Still no valid lat/lon points after cleaning. Verify column names/values.\")\n",
    "\n",
    "    return g\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Time Analysis Visualizations\n",
    "# =========================\n",
    "def create_time_analysis_plots(data: pd.DataFrame) -> go.Figure:\n",
    "    \"\"\"Create time-based analytics: by hour, weekday, month, and pie for time-of-day.\"\"\"\n",
    "    data = ensure_time_features(data)\n",
    "\n",
    "    # 1) Crimes by hour\n",
    "    hourly_crimes = data.groupby(HOUR_COL).size().reindex(range(24), fill_value=0)\n",
    "\n",
    "    # 2) Crimes by weekday (fixed order)\n",
    "    daily_crimes = data.groupby(DAYOFWEEK_COL).size()\n",
    "    daily_crimes = daily_crimes.reindex(DAILY_ORDER).fillna(0)\n",
    "\n",
    "    # 3) Monthly trend\n",
    "    monthly_crimes = data.groupby(MONTH_COL).size().sort_index()\n",
    "\n",
    "    # 4) Time of day (optional)\n",
    "    if TIMEOFDAY_COL in data.columns:\n",
    "        time_of_day_crimes = data[TIMEOFDAY_COL].fillna(\"Unknown\").value_counts()\n",
    "    else:\n",
    "        time_of_day_crimes = pd.Series(dtype=int)\n",
    "\n",
    "    # Subplots: Pie chart in bottom-right cell\n",
    "    fig = make_subplots(\n",
    "        rows=2, cols=2,\n",
    "        subplot_titles=(\n",
    "            \"Crimes by Hour of Day\",\n",
    "            \"Crimes by Day of Week\",\n",
    "            \"Crimes by Month\",\n",
    "            \"Crimes by Time of Day\"\n",
    "        ),\n",
    "        specs=[[{\"type\": \"xy\"}, {\"type\": \"xy\"}],\n",
    "               [{\"type\": \"xy\"}, {\"type\": \"domain\"}]]\n",
    "    )\n",
    "\n",
    "    # Hourly crimes (line chart)\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=list(hourly_crimes.index), y=hourly_crimes.values,\n",
    "                   mode=\"lines+markers\", name=\"Hourly\"),\n",
    "        row=1, col=1\n",
    "    )\n",
    "\n",
    "    # Daily crimes (bar chart)\n",
    "    fig.add_trace(\n",
    "        go.Bar(x=daily_crimes.index.tolist(), y=daily_crimes.values, name=\"Daily\"),\n",
    "        row=1, col=2\n",
    "    )\n",
    "\n",
    "    # Monthly crimes (line chart)\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=monthly_crimes.index.tolist(), y=monthly_crimes.values,\n",
    "                   mode=\"lines+markers\", name=\"Monthly\"),\n",
    "        row=2, col=1\n",
    "    )\n",
    "\n",
    "    # Time of day (pie chart) if available\n",
    "    if len(time_of_day_crimes) > 0:\n",
    "        fig.add_trace(\n",
    "            go.Pie(labels=time_of_day_crimes.index.astype(str),\n",
    "                   values=time_of_day_crimes.values,\n",
    "                   textinfo=\"label+percent\",\n",
    "                   name=\"Time of Day\"),\n",
    "            row=2, col=2\n",
    "        )\n",
    "\n",
    "    fig.update_layout(\n",
    "        height=820,\n",
    "        title_text=\"CityX ‚Äî Time-based Crime Analysis\",\n",
    "        showlegend=False,\n",
    "        margin=dict(l=50, r=50, t=80, b=50)\n",
    "    )\n",
    "    return fig\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Interactive Map (Plotly Mapbox)\n",
    "# =========================\n",
    "def create_crime_map(df: pd.DataFrame) -> px.scatter_mapbox:\n",
    "    \"\"\"Create an interactive map with automatic cleaning and lat/lon fixing.\"\"\"\n",
    "    g = fix_and_filter_geo(df)\n",
    "\n",
    "    # Determine map center\n",
    "    center_lat = float(g[LAT_COL].median())\n",
    "    center_lon = float(g[LON_COL].median())\n",
    "\n",
    "    # Prepare hover columns\n",
    "    hover_cols = [c for c in [DESCRIPT_COL, PDDIST_COL] + DATE_COLS + [\"Address\"] if c in g.columns]\n",
    "\n",
    "    fig_map = px.scatter_mapbox(\n",
    "        g,\n",
    "        lat=LAT_COL, lon=LON_COL,\n",
    "        color=CATEGORY_COL if CATEGORY_COL in g.columns else None,\n",
    "        hover_name=CATEGORY_COL if CATEGORY_COL in g.columns else None,\n",
    "        hover_data=hover_cols,\n",
    "        opacity=0.75,\n",
    "        height=700,\n",
    "        zoom=11,\n",
    "        mapbox_style=\"carto-positron\"  # No access token required\n",
    "    )\n",
    "    fig_map.update_layout(mapbox_center={\"lat\": center_lat, \"lon\": center_lon})\n",
    "    fig_map.update_traces(marker=dict(size=6))\n",
    "    return fig_map\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Run and save outputs\n",
    "# =========================\n",
    "def build_time_and_map_html(data: pd.DataFrame,\n",
    "                            time_html_path: str = \"time_analysis.html\",\n",
    "                            map_html_path: str = \"crime_map.html\"):\n",
    "    \"\"\"Builds HTML files for time analytics and interactive map.\"\"\"\n",
    "    print(\"Creating time analysis figure...\")\n",
    "    time_fig = create_time_analysis_plots(data)\n",
    "    time_fig.write_html(time_html_path)\n",
    "    print(f\"Time analysis saved ‚Üí {time_html_path}\")\n",
    "\n",
    "    print(\"Creating interactive crime map...\")\n",
    "    map_fig = create_crime_map(data)\n",
    "    map_fig.write_html(map_html_path)\n",
    "    print(f\"Crime map saved ‚Üí {map_html_path}\")\n",
    "\n",
    "    return time_fig, map_fig\n",
    "\n",
    "\n",
    "# ========== Example usage ==========\n",
    "time_fig, map_fig = build_time_and_map_html(DATA)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4cc990da-cd6e-40c8-b6e5-6b82de7ca9c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cityx_crime_dashboard.py\n"
     ]
    }
   ],
   "source": [
    "# Write the file using a raw triple-single-quoted string to avoid quote conflicts\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "code = r'''import streamlit as st\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import folium\n",
    "from folium.plugins import HeatMap, MarkerCluster\n",
    "from streamlit_folium import folium_static\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import datetime\n",
    "\n",
    "# -------------------- Page config --------------------\n",
    "st.set_page_config(\n",
    "    page_title=\"CityX Crime Watch Dashboard\", \n",
    "    page_icon=\"üöî\", \n",
    "    layout=\"wide\",\n",
    "    initial_sidebar_state=\"expanded\"\n",
    ")\n",
    "\n",
    "# -------------------- Simplified Map Display --------------------\n",
    "def show_map(m, width=1200, height=600):\n",
    "    \"\"\"Display Folium map reliably using static method\"\"\"\n",
    "    try:\n",
    "        folium_static(m, width=width, height=height)\n",
    "    except Exception as e:\n",
    "        st.error(f\"Map display error: {str(e)}\")\n",
    "        st.info(\"Try checking your coordinate data in the debug section below\")\n",
    "\n",
    "# -------------------- Smart Data Processing --------------------\n",
    "COMMON_DATE_COLS = [\"Dates\", \"Date\", \"Datetime\", \"datetime\", \"timestamp\", \"Timestamp\", \"time\", \"Time\"]\n",
    "# Support common and varying coordinate column names\n",
    "COMMON_LAT_COLS = [\"Latitude\", \"LATITUDE\", \"lat\", \"Lat\", \"Y\", \"y\", \"latitude\", \"Latitude (Y)\"]\n",
    "COMMON_LON_COLS = [\"Longitude\", \"LONGITUDE\", \"lon\", \"Lon\", \"lng\", \"Lng\", \"Long\", \"long\", \"X\", \"x\", \"longitude\", \"Longitude (X)\"]\n",
    "COMMON_CATEGORY_COLS = [\"Category\", \"category\", \"Type\", \"type\", \"Offense\", \"offense\", \"Crime Type\"]\n",
    "COMMON_DISTRICT_COLS = [\"PdDistrict\", \"District\", \"district\", \"Division\", \"division\", \"Precinct\", \"precinct\"]\n",
    "\n",
    "def _normalize_colname(s: str) -> str:\n",
    "    \"\"\"lower + trim inner spaces for robust matching\"\"\"\n",
    "    return \" \".join(str(s).strip().split()).lower()\n",
    "\n",
    "def detect_column(df, possible_names):\n",
    "    \"\"\"\n",
    "    Robust column detection:\n",
    "    - case-insensitive\n",
    "    - trims spaces\n",
    "    - tries smart hints for lat/lon variants\n",
    "    \"\"\"\n",
    "    normalized_map = { _normalize_colname(c): c for c in df.columns }\n",
    "    candidates = set(_normalize_colname(n) for n in possible_names)\n",
    "\n",
    "    # direct match\n",
    "    for norm, original in normalized_map.items():\n",
    "        if norm in candidates:\n",
    "            return original\n",
    "\n",
    "    # smart hints for latitude-like\n",
    "    for norm, original in normalized_map.items():\n",
    "        if any(k in norm for k in [\"latitude (y)\", \"lat (y)\", \" lat \", \"(y)\", \" y \"]):\n",
    "            return original\n",
    "    # smart hints for longitude-like\n",
    "    for norm, original in normalized_map.items():\n",
    "        if any(k in norm for k in [\"longitude (x)\", \"lon (x)\", \" long \", \"(x)\", \" x \"]):\n",
    "            return original\n",
    "\n",
    "    # fallback contains\n",
    "    for norm, original in normalized_map.items():\n",
    "        if \"lat\" in norm:\n",
    "            return original\n",
    "    for norm, original in normalized_map.items():\n",
    "        if \"lon\" in norm or \"long\" in norm or \"lng\" in norm:\n",
    "            return original\n",
    "\n",
    "    return None\n",
    "\n",
    "def smart_data_processing(df, skip_coord_filtering=True):\n",
    "    \"\"\"Process data with intelligent column detection and cleaning\"\"\"\n",
    "    \n",
    "    # Store original info for debugging\n",
    "    original_info = {\n",
    "        'columns': df.columns.tolist(),\n",
    "        'original_shape': df.shape,\n",
    "        'detected_columns': {}\n",
    "    }\n",
    "    \n",
    "    # Detect and standardize columns\n",
    "    df_clean = df.copy()\n",
    "    \n",
    "    # Date detection and processing\n",
    "    date_col = detect_column(df, COMMON_DATE_COLS)\n",
    "    if date_col:\n",
    "        df_clean['Date'] = pd.to_datetime(df_clean[date_col], errors='coerce')\n",
    "        df_clean['Hour'] = df_clean['Date'].dt.hour.fillna(12).astype(int)\n",
    "        df_clean['Month'] = df_clean['Date'].dt.month.fillna(1).astype(int)\n",
    "        df_clean['Year'] = df_clean['Date'].dt.year.fillna(datetime.datetime.now().year).astype(int)\n",
    "        df_clean['DayOfWeek'] = df_clean['Date'].dt.day_name().fillna('Unknown')\n",
    "        original_info['detected_columns']['date'] = date_col\n",
    "    else:\n",
    "        # Fallback defaults if no usable date column is found\n",
    "        df_clean['Hour'] = 12\n",
    "        df_clean['Month'] = 1\n",
    "        df_clean['Year'] = datetime.datetime.now().year\n",
    "        df_clean['DayOfWeek'] = 'Unknown'\n",
    "    \n",
    "    # Coordinate detection and processing\n",
    "    lat_col = detect_column(df, COMMON_LAT_COLS + [\"Latitude (Y)\", \"Y\"])\n",
    "    lon_col = detect_column(df, COMMON_LON_COLS + [\"Longitude (X)\", \"X\"])\n",
    "    \n",
    "    if lat_col and lon_col:\n",
    "        # 1) Text cleaning: strip spaces and replace Arabic comma with dot\n",
    "        lat_raw = df_clean[lat_col].astype(str).str.strip().str.replace(\",\", \".\", regex=False)\n",
    "        lon_raw = df_clean[lon_col].astype(str).str.strip().str.replace(\",\", \".\", regex=False)\n",
    "\n",
    "        # 2) Convert to numeric\n",
    "        lat_num = pd.to_numeric(lat_raw, errors='coerce')\n",
    "        lon_num = pd.to_numeric(lon_raw, errors='coerce')\n",
    "\n",
    "        # 3) Check if coordinates are swapped (swap heuristic)\n",
    "        normal_valid = (lat_num.between(-90, 90) & lon_num.between(-180, 180)).sum()\n",
    "        swapped_valid = (lon_num.between(-90, 90) & lat_num.between(-180, 180)).sum()\n",
    "\n",
    "        if swapped_valid > normal_valid * 1.5:\n",
    "            # Swap if improvement is clear\n",
    "            df_clean['Latitude']  = lon_num\n",
    "            df_clean['Longitude'] = lat_num\n",
    "            original_info['detected_columns']['latitude']  = lat_col + \" (swapped)\"\n",
    "            original_info['detected_columns']['longitude'] = lon_col + \" (swapped)\"\n",
    "        else:\n",
    "            df_clean['Latitude']  = lat_num\n",
    "            df_clean['Longitude'] = lon_num\n",
    "            original_info['detected_columns']['latitude']  = lat_col\n",
    "            original_info['detected_columns']['longitude'] = lon_col\n",
    "        \n",
    "        if not skip_coord_filtering:\n",
    "            # Keep only valid coordinate ranges\n",
    "            valid_coords = (\n",
    "                df_clean['Latitude'].between(-90, 90) & \n",
    "                df_clean['Longitude'].between(-180, 180)\n",
    "            )\n",
    "            df_clean = df_clean[valid_coords].copy()\n",
    "    else:\n",
    "        # Create standard empty columns to avoid downstream errors\n",
    "        if 'Latitude' not in df_clean.columns:  df_clean['Latitude'] = np.nan\n",
    "        if 'Longitude' not in df_clean.columns: df_clean['Longitude'] = np.nan\n",
    "    \n",
    "    # Category detection\n",
    "    category_col = detect_column(df, COMMON_CATEGORY_COLS)\n",
    "    if category_col:\n",
    "        df_clean['Category'] = df_clean[category_col].astype(str)\n",
    "        original_info['detected_columns']['category'] = category_col\n",
    "    \n",
    "    # District detection  \n",
    "    district_col = detect_column(df, COMMON_DISTRICT_COLS)\n",
    "    if district_col:\n",
    "        df_clean['District'] = df_clean[district_col].astype(str)\n",
    "        original_info['detected_columns']['district'] = district_col\n",
    "    \n",
    "    original_info['final_shape'] = df_clean.shape\n",
    "    original_info['rows_with_coords'] = (\n",
    "        df_clean[['Latitude', 'Longitude']].notna().all(axis=1).sum()\n",
    "        if 'Latitude' in df_clean.columns and 'Longitude' in df_clean.columns\n",
    "        else 0\n",
    "    )\n",
    "    \n",
    "    return df_clean, original_info\n",
    "\n",
    "\n",
    "# -------------------- Dashboard Setup --------------------\n",
    "st.sidebar.title(\"üöî CityX Crime Watch\")\n",
    "st.sidebar.markdown(\"---\")\n",
    "\n",
    "# Data Source Section\n",
    "st.sidebar.header(\"Data Source\")\n",
    "data_source = st.sidebar.radio(\"Choose data source:\", \n",
    "                              [\"Use Competition Dataset\", \"Upload CSV\"], \n",
    "                              index=0)  # Default choice\n",
    "\n",
    "df = None\n",
    "data_info = {}\n",
    "\n",
    "if data_source == \"Use Competition Dataset\":\n",
    "    try:\n",
    "        raw_df = pd.read_csv(\"../data/raw/Competition_Dataset.csv\")\n",
    "        skip_filter = st.sidebar.checkbox(\"Skip coordinate filtering\", value=True)\n",
    "        df, data_info = smart_data_processing(raw_df, skip_coord_filtering=skip_filter)\n",
    "        st.sidebar.success(f\"Loaded {len(df):,} records from Competition_Dataset.csv\")\n",
    "    except Exception as e:\n",
    "        st.sidebar.error(f\"Error loading Competition_Dataset.csv: {str(e)}\")\n",
    "        st.stop()\n",
    "\n",
    "elif data_source == \"Upload CSV\":\n",
    "    uploaded_file = st.sidebar.file_uploader(\"Choose CSV file\", type=\"csv\")\n",
    "    if uploaded_file is not None:\n",
    "        try:\n",
    "            raw_df = pd.read_csv(uploaded_file)\n",
    "            skip_filter = st.sidebar.checkbox(\"Skip coordinate filtering\", value=True)\n",
    "            df, data_info = smart_data_processing(raw_df, skip_coord_filtering=skip_filter)\n",
    "            st.sidebar.success(f\"Loaded {len(df):,} records\")\n",
    "        except Exception as e:\n",
    "            st.sidebar.error(f\"Error loading file: {str(e)}\")\n",
    "    else:\n",
    "        st.info(\"üëÜ Please upload a CSV file or switch to another data source\")\n",
    "        st.stop()\n",
    "\n",
    "if df is None or df.empty:\n",
    "    st.error(\"No data available. Please check your data source.\")\n",
    "    st.stop()\n",
    "\n",
    "# -------------------- Filters --------------------\n",
    "st.sidebar.header(\"üîç Filters\")\n",
    "st.sidebar.markdown(\"---\")\n",
    "\n",
    "# Year Filter\n",
    "available_years = sorted(df['Year'].unique())\n",
    "if len(available_years) > 0:\n",
    "    year_range = st.sidebar.slider(\n",
    "        \"Year Range\",\n",
    "        min_value=int(min(available_years)),\n",
    "        max_value=int(max(available_years)),\n",
    "        value=(int(min(available_years)), int(max(available_years))) )\n",
    "else:\n",
    "    year_range = (2023, 2023)\n",
    "\n",
    "# Hour Filter  \n",
    "hour_range = st.sidebar.slider(\"Hour of Day\", 0, 23, (0, 23))\n",
    "\n",
    "# Category Filter\n",
    "if 'Category' in df.columns:\n",
    "    categories = sorted(df['Category'].unique())\n",
    "    selected_categories = st.sidebar.multiselect(\n",
    "        \"Crime Categories\",\n",
    "        options=categories,\n",
    "        #default=categories[:min(5, len(categories))]\n",
    "\n",
    "    )\n",
    "else:\n",
    "    selected_categories = []\n",
    "\n",
    "# District Filter\n",
    "if 'District' in df.columns:\n",
    "    districts = sorted(df['District'].unique())\n",
    "    selected_districts = st.sidebar.multiselect(\n",
    "        \"Police Districts\", \n",
    "        options=districts,\n",
    "        default=districts\n",
    "    )\n",
    "else:\n",
    "    selected_districts = []\n",
    "\n",
    "# Apply Filters\n",
    "filter_mask = (\n",
    "    df['Year'].between(year_range[0], year_range[1]) & \n",
    "    df['Hour'].between(hour_range[0], hour_range[1])\n",
    ")\n",
    "\n",
    "if selected_categories:\n",
    "    filter_mask &= df['Category'].isin(selected_categories)\n",
    "if selected_districts:\n",
    "    filter_mask &= df['District'].isin(selected_districts)\n",
    "\n",
    "filtered_df = df[filter_mask].copy()\n",
    "\n",
    "# -------------------- Main Dashboard --------------------\n",
    "st.title(\"üöî CityX Crime Watch: Operation Safe Streets....\")\n",
    "st.markdown(\"Real-time crime analysis and visualization dashboard\")\n",
    "st.markdown(\"---\")\n",
    "\n",
    "# Key Metrics\n",
    "col1, col2, col3, col4 = st.columns(4)\n",
    "\n",
    "with col1:\n",
    "    total_incidents = len(filtered_df)\n",
    "    st.metric(\"Total Incidents\", f\"{total_incidents:,}\")\n",
    "\n",
    "with col2:\n",
    "    unique_categories = filtered_df['Category'].nunique() if 'Category' in filtered_df.columns else 0\n",
    "    st.metric(\"Crime Categories\", unique_categories)\n",
    "\n",
    "with col3:\n",
    "    if 'District' in filtered_df.columns and not filtered_df.empty:\n",
    "        top_district = filtered_df['District'].mode()[0] if not filtered_df['District'].mode().empty else \"N/A\"\n",
    "    else:\n",
    "        top_district = \"N/A\"\n",
    "    st.metric(\"Top District\", top_district)\n",
    "\n",
    "with col4:\n",
    "    if 'Category' in filtered_df.columns and not filtered_df.empty:\n",
    "        top_crime = filtered_df['Category'].mode()[0] if not filtered_df['Category'].mode().empty else \"N/A\"\n",
    "    else:\n",
    "        top_crime = \"N/A\"\n",
    "    st.metric(\"Most Common Crime\", top_crime)\n",
    "\n",
    "# -------------------- Main Tabs --------------------\n",
    "tab1, tab2, tab3, tab4 = st.tabs([\"üåç Crime Heatmap\", \"üìä Crime Clusters\", \"üìà Analytics\", \"üëÆ District Analysis\"])\n",
    "\n",
    "with tab1:\n",
    "    st.subheader(\"Crime Density Heatmap\")\n",
    "    \n",
    "    # Always use the standardized columns\n",
    "    if 'Latitude' in filtered_df.columns and 'Longitude' in filtered_df.columns:\n",
    "        coords = filtered_df[['Latitude', 'Longitude']].copy()\n",
    "        # Safety: numeric conversion here as well\n",
    "        coords['Latitude'] = pd.to_numeric(coords['Latitude'], errors='coerce')\n",
    "        coords['Longitude'] = pd.to_numeric(coords['Longitude'], errors='coerce')\n",
    "\n",
    "        valid_coords = coords.dropna()\n",
    "        valid_coords = valid_coords[\n",
    "            (valid_coords['Latitude'].between(-90, 90)) & \n",
    "            (valid_coords['Longitude'].between(-180, 180))\n",
    "        ]\n",
    "        \n",
    "        if not valid_coords.empty:\n",
    "            # Heatmap controls\n",
    "            col1, col2, col3 = st.columns(3)\n",
    "            with col1:\n",
    "                radius = st.slider(\"Heat Radius\", 5, 25, 12)\n",
    "            with col2:\n",
    "                blur = st.slider(\"Blur Intensity\", 5, 25, 15)\n",
    "            with col3:\n",
    "                max_zoom = st.slider(\"Max Zoom Level\", 10, 18, 13)\n",
    "            \n",
    "            # Create map\n",
    "            center_lat = valid_coords['Latitude'].mean()\n",
    "            center_lon = valid_coords['Longitude'].mean()\n",
    "            \n",
    "            m = folium.Map(\n",
    "                location=[center_lat, center_lon],\n",
    "                zoom_start=12,\n",
    "                tiles='CartoDB positron',\n",
    "                control_scale=True\n",
    "            )\n",
    "            \n",
    "            # Add heatmap\n",
    "            heat_data = valid_coords[['Latitude', 'Longitude']].values.tolist()\n",
    "            HeatMap(\n",
    "                heat_data, \n",
    "                radius=radius, \n",
    "                blur=blur, \n",
    "                max_zoom=max_zoom,\n",
    "                gradient={0.4: 'blue', 0.65: 'lime', 0.8: 'yellow', 1.0: 'red'}\n",
    "            ).add_to(m)\n",
    "            \n",
    "            # Fit bounds (names are correct now)\n",
    "            m.fit_bounds([\n",
    "                [valid_coords['Latitude'].min(), valid_coords['Longitude'].min()],\n",
    "                [valid_coords['Latitude'].max(), valid_coords['Longitude'].max()]\n",
    "            ])\n",
    "            \n",
    "            show_map(m)\n",
    "            \n",
    "            # Stats\n",
    "            st.info(f\"üìç Displaying {len(valid_coords):,} incidents on map\")\n",
    "        else:\n",
    "            st.warning(\"No valid coordinates available for mapping. Try enabling 'Skip coordinate filtering' in sidebar.\")\n",
    "    else:\n",
    "        st.error(\"No coordinate data found. Please ensure your data contains latitude and longitude columns.\")\n",
    "\n",
    "with tab2:\n",
    "    st.subheader(\"Crime Clusters by Category\")\n",
    "    \n",
    "    if 'Category' in filtered_df.columns and 'Latitude' in filtered_df.columns and 'Longitude' in filtered_df.columns:\n",
    "        valid_data = filtered_df[['Latitude', 'Longitude', 'Category']].dropna()\n",
    "        valid_data = valid_data[\n",
    "            (valid_data['Latitude'].between(-90, 90)) & \n",
    "            (valid_data['Longitude'].between(-180, 180))\n",
    "        ]\n",
    "        \n",
    "        if not valid_data.empty:\n",
    "            # Get top categories for coloring\n",
    "            top_categories = valid_data['Category'].value_counts().head(8).index.tolist()\n",
    "            \n",
    "            # Create map\n",
    "            center_lat = valid_data['Latitude'].mean()\n",
    "            center_lon = valid_data['Longitude'].mean()\n",
    "            \n",
    "            m = folium.Map(\n",
    "                location=[center_lat, center_lon],\n",
    "                zoom_start=12,\n",
    "                tiles='CartoDB positron'\n",
    "            )\n",
    "            \n",
    "            # Color palette\n",
    "            colors = ['red', 'blue', 'green', 'purple', 'orange', 'darkred', 'lightred', 'darkblue']\n",
    "            \n",
    "            # Add clustered markers for each category\n",
    "            for i, category in enumerate(top_categories):\n",
    "                color = colors[i % len(colors)]\n",
    "                category_data = valid_data[valid_data['Category'] == category]\n",
    "                \n",
    "                # Create feature group\n",
    "                feature_group = folium.FeatureGroup(name=f\"{category} ({len(category_data)})\")\n",
    "                \n",
    "                # Add marker cluster\n",
    "                marker_cluster = MarkerCluster().add_to(feature_group)\n",
    "                \n",
    "                # Sample for performance\n",
    "                sample_size = min(500, len(category_data))\n",
    "                sample_data = category_data.sample(n=sample_size, random_state=42)\n",
    "                \n",
    "                for _, row in sample_data.iterrows():\n",
    "                    folium.CircleMarker(\n",
    "                        location=[row['Latitude'], row['Longitude']],\n",
    "                        radius=4,\n",
    "                        color=color,\n",
    "                        fill=True,\n",
    "                        fillOpacity=0.7,\n",
    "                        popup=folium.Popup(f\"<b>{category}</b>\", max_width=200)\n",
    "                    ).add_to(marker_cluster)\n",
    "                \n",
    "                m.add_child(feature_group)\n",
    "            \n",
    "            # Add layer control\n",
    "            folium.LayerControl(collapsed=False).add_to(m)\n",
    "            m.fit_bounds(valid_data[['Latitude', 'Longitude']].values.tolist())\n",
    "            \n",
    "            show_map(m)\n",
    "            \n",
    "            # Category summary\n",
    "            st.subheader(\"Category Summary\")\n",
    "            category_summary = valid_data['Category'].value_counts()\n",
    "            fig = px.bar(\n",
    "                x=category_summary.values, \n",
    "                y=category_summary.index,\n",
    "                orientation='h',\n",
    "                title=\"Crimes by Category\",\n",
    "                labels={'x': 'Number of Incidents', 'y': 'Category'}\n",
    "            )\n",
    "            st.plotly_chart(fig, use_container_width=True)\n",
    "            \n",
    "        else:\n",
    "            st.warning(\"No valid data available for clustering.\")\n",
    "    else:\n",
    "        st.error(\"Required columns (Category, Latitude, Longitude) not found.\")\n",
    "\n",
    "with tab3:\n",
    "    st.subheader(\"Crime Analytics & Trends\")\n",
    "    \n",
    "    if not filtered_df.empty:\n",
    "        # Create subplots grid\n",
    "        fig = make_subplots(\n",
    "            rows=2, cols=2,\n",
    "            subplot_titles=('Crimes by Hour of Day', 'Crimes by Month', 'Top Crime Categories', 'Crimes by District'),\n",
    "            specs=[[{\"type\": \"bar\"}, {\"type\": \"bar\"}], [{\"type\": \"bar\"}, {\"type\": \"bar\"}]]\n",
    "        )\n",
    "        \n",
    "        # Hourly distribution\n",
    "        hourly_counts = filtered_df['Hour'].value_counts().sort_index()\n",
    "        fig.add_trace(\n",
    "            go.Bar(x=hourly_counts.index, y=hourly_counts.values, name='By Hour'),\n",
    "            row=1, col=1\n",
    "        )\n",
    "        \n",
    "        # Monthly distribution\n",
    "        if 'Month' in filtered_df.columns:\n",
    "            monthly_counts = filtered_df['Month'].value_counts().sort_index()\n",
    "            fig.add_trace(\n",
    "                go.Bar(x=monthly_counts.index, y=monthly_counts.values, name='By Month'),\n",
    "                row=1, col=2\n",
    "            )\n",
    "        \n",
    "        # Top categories\n",
    "        if 'Category' in filtered_df.columns:\n",
    "            top_cats = filtered_df['Category'].value_counts().head(10)\n",
    "            fig.add_trace(\n",
    "                go.Bar(x=top_cats.values, y=top_cats.index, orientation='h', name='Top Categories'),\n",
    "                row=2, col=1\n",
    "            )\n",
    "        \n",
    "        # District distribution\n",
    "        if 'District' in filtered_df.columns:\n",
    "            district_counts = filtered_df['District'].value_counts()\n",
    "            fig.add_trace(\n",
    "                go.Bar(x=district_counts.index, y=district_counts.values, name='By District'),\n",
    "                row=2, col=2\n",
    "            )\n",
    "        \n",
    "        fig.update_layout(height=600, showlegend=False, title_text=\"Crime Analysis Overview\")\n",
    "        st.plotly_chart(fig, use_container_width=True)\n",
    "        \n",
    "        # Additional insights\n",
    "        col1, col2 = st.columns(2)\n",
    "        \n",
    "        with col1:\n",
    "            if 'DayOfWeek' in filtered_df.columns:\n",
    "                day_counts = filtered_df['DayOfWeek'].value_counts()\n",
    "                fig_dow = px.pie(\n",
    "                    values=day_counts.values, \n",
    "                    names=day_counts.index,\n",
    "                    title=\"Crimes by Day of Week\"\n",
    "                )\n",
    "                st.plotly_chart(fig_dow, use_container_width=True)\n",
    "        \n",
    "        with col2:\n",
    "            if 'Year' in filtered_df.columns:\n",
    "                yearly_trend = filtered_df['Year'].value_counts().sort_index()\n",
    "                fig_trend = px.line(\n",
    "                    x=yearly_trend.index, \n",
    "                    y=yearly_trend.values,\n",
    "                    title=\"Yearly Crime Trend\",\n",
    "                    labels={'x': 'Year', 'y': 'Incidents'}\n",
    "                )\n",
    "                st.plotly_chart(fig_trend, use_container_width=True)\n",
    "                \n",
    "    else:\n",
    "        st.info(\"No data available for analysis with current filters.\")\n",
    "\n",
    "with tab4:\n",
    "    st.subheader(\"Police District Analysis\")\n",
    "    \n",
    "    if 'District' in filtered_df.columns and 'Latitude' in filtered_df.columns and 'Longitude' in filtered_df.columns:\n",
    "        valid_data = filtered_df[['Latitude', 'Longitude', 'District']].dropna()\n",
    "        valid_data = valid_data[\n",
    "            (valid_data['Latitude'].between(-90, 90)) & \n",
    "            (valid_data['Longitude'].between(-180, 180))\n",
    "        ]\n",
    "        \n",
    "        if not valid_data.empty:\n",
    "            # District statistics summary\n",
    "            district_stats = filtered_df.groupby('District').agg({\n",
    "                'Hour': ['mean', 'std'],\n",
    "                'Category': 'count'\n",
    "            }).round(2)\n",
    "            \n",
    "            district_stats.columns = ['Avg Hour', 'Hour Std', 'Total Crimes']\n",
    "            district_stats = district_stats.sort_values('Total Crimes', ascending=False)\n",
    "            \n",
    "            # Create district map\n",
    "            center_lat = valid_data['Latitude'].mean()\n",
    "            center_lon = valid_data['Longitude'].mean()\n",
    "            \n",
    "            m = folium.Map(\n",
    "                location=[center_lat, center_lon],\n",
    "                zoom_start=11,\n",
    "                tiles='CartoDB positron'\n",
    "            )\n",
    "            \n",
    "            # Add district centers (circle markers sized by count)\n",
    "            for district in district_stats.index:\n",
    "                district_data = valid_data[valid_data['District'] == district]\n",
    "                if not district_data.empty:\n",
    "                    center_lat_dist = district_data['Latitude'].mean()\n",
    "                    center_lon_dist = district_data['Longitude'].mean()\n",
    "                    count = len(district_data)\n",
    "                    \n",
    "                    folium.CircleMarker(\n",
    "                        location=[center_lat_dist, center_lon_dist],\n",
    "                        radius=max(10, min(50, count / 50)),\n",
    "                        popup=folium.Popup(\n",
    "                            f\"<b>{district} District</b><br>\"\n",
    "                            f\"Total Crimes: {count:,}<br>\"\n",
    "                            f\"Avg Hour: {district_stats.loc[district, 'Avg Hour']}\",\n",
    "                            max_width=250\n",
    "                        ),\n",
    "                        color='blue',\n",
    "                        fill=True,\n",
    "                        fillOpacity=0.6,\n",
    "                        fillColor='lightblue'\n",
    "                    ).add_to(m)\n",
    "            \n",
    "            show_map(m)\n",
    "            \n",
    "            # District statistics table\n",
    "            st.subheader(\"District Performance Metrics\")\n",
    "            st.dataframe(district_stats, use_container_width=True)\n",
    "            \n",
    "            # District comparison chart\n",
    "            fig_dist = px.bar(\n",
    "                x=district_stats.index,\n",
    "                y=district_stats['Total Crimes'],\n",
    "                title=\"Crime Distribution by District\",\n",
    "                labels={'x': 'District', 'y': 'Number of Crimes'}\n",
    "            )\n",
    "            st.plotly_chart(fig_dist, use_container_width=True)\n",
    "            \n",
    "        else:\n",
    "            st.warning(\"No valid coordinate data available for district analysis.\")\n",
    "    else:\n",
    "        st.error(\"District or coordinate data not available.\")\n",
    "\n",
    "\n",
    "# -------------------- Footer --------------------\n",
    "st.markdown(\"---\")\n",
    "st.markdown(\n",
    "    \"**CityX Crime Watch Dashboard** | \"\n",
    "    \"Real-time crime analysis and visualization | \"\n",
    "    \"For official use only\"\n",
    ")\n",
    "'''\n",
    "\n",
    "out = Path(\"cityx_crime_dashboard.py\")\n",
    "out.parent.mkdir(parents=True, exist_ok=True)\n",
    "out.write_text(code, encoding=\"utf-8\")\n",
    "print(str(out))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "273f7f4a-3724-40bd-b247-51f299eee1ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5938e062-786b-4ddd-b613-876fea4c329b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
